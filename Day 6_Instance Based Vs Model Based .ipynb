{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cea3d5-1921-410f-bec0-3ef5e9671f49",
   "metadata": {},
   "source": [
    "### Types of ML based on how algorithm learns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a8717-eccc-4e9a-8612-3232fa7eeaa5",
   "metadata": {},
   "source": [
    "Learning:\n",
    "1. Memorizing\n",
    "2. Generalizing (Undestanding Concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94912ef3-19dd-421e-9d86-bb411f3359e7",
   "metadata": {},
   "source": [
    "Let's clarify the distinction between memorizing (which often involves overfitting or learning specifics without generalization) and generalizing (which involves understanding the underlying patterns and applying them to new, unseen data). Here's how different algorithms fit into these categories:\n",
    "\n",
    "##### 1. Memorizing Algorithms\n",
    "These algorithms tend to remember the training data very well, sometimes at the cost of generalization. They can perform exceptionally well on the training set but may struggle with unseen data due to overfitting.\n",
    "\n",
    "k-Nearest Neighbors (k-NN): This is a classic example of a memorizing algorithm. It stores the entire training dataset and makes predictions based on the closest data points, often leading to poor generalization if the training data is noisy or not representative.\n",
    "\n",
    "Decision Trees (without pruning): If a decision tree is allowed to grow without constraints, it can memorize the training data by creating very specific rules for each example, leading to overfitting.\n",
    "\n",
    "Overfitted Neural Networks: When a neural network is trained too long without proper regularization, it can memorize the training data, including noise, rather than learning general patterns.\n",
    "\n",
    "#### 2. Generalizing Algorithms (Understanding Concepts)\n",
    "These algorithms focus on understanding the underlying patterns in the data and generalize well to new, unseen data. They strike a balance between fitting the training data and maintaining flexibility to apply learned concepts to different datasets.\n",
    "\n",
    "Linear Regression: By fitting a linear equation to the data, this algorithm captures general trends without overfitting, especially when regularized.\n",
    "\n",
    "Support Vector Machines (SVMs): SVMs aim to find a hyperplane that maximizes the margin between different classes, which helps in generalization, especially with the right kernel.\n",
    "\n",
    "Random Forests: By averaging the predictions of multiple decision trees, Random Forests reduce overfitting and generalize better compared to a single decision tree.\n",
    "\n",
    "Neural Networks (with regularization): Techniques like dropout, L2 regularization, and early stopping help neural networks learn general patterns rather than memorizing the training data.\n",
    "\n",
    "Regularized Logistic Regression: This model generalizes well by applying a penalty to large coefficients, preventing the model from relying too heavily on any single feature.\n",
    "\n",
    "Gradient Boosting Machines (e.g., XGBoost, LightGBM): These models build an ensemble of weak learners (often decision trees) that correct errors of previous models, focusing on generalization through iterative learning.\n",
    "\n",
    "### Hybrid or Balanced Approaches\n",
    "##### Some algorithms and techniques are designed to balance memorization and generalization:\n",
    "\n",
    "Cross-Validation: This technique helps in evaluating a model's ability to generalize by testing it on multiple subsets of the data.\n",
    "\n",
    "Ensemble Methods (e.g., Bagging, Boosting): Combining multiple models can help balance memorization and generalization, leveraging the strengths of different algorithms.\n",
    "\n",
    "Transfer Learning: In neural networks, transfer learning involves using a pre-trained model on a similar problem, which helps in generalizing by leveraging learned representations from other datasets.\n",
    "\n",
    "These approaches help in understanding concepts and generalizing them to unseen data, which is essential for building robust machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de83ef-6e1b-4a06-aad9-917581fb4d4c",
   "metadata": {},
   "source": [
    "#### 1. Instance Based Learning  / Lazy learner\n",
    "- Does nothing in training time \n",
    "- As soon as new data comes it reacts\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b00a88-c9a1-427d-b0fa-9b432b3e6559",
   "metadata": {},
   "source": [
    "#### 2. Model Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9a41c-ce8a-43ba-8a85-83dd2766571b",
   "metadata": {},
   "source": [
    "- Tries to understand the behaviour data\n",
    "- draws boundry\n",
    "- Tries to find mathemathically function between input and output variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0999a-ea7a-4e6f-9fcd-debdbc0fb673",
   "metadata": {},
   "source": [
    "![](InstanceVsModel.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7685b0-eff2-4743-a7b3-bfbfe9d469b1",
   "metadata": {},
   "source": [
    "Link: https://youtu.be/ntAOq1ioTKo?si=Xawww_Sk6lVbzL3O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd3680a-e868-482b-9d61-a85ff0552643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![](filename.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42d720-e8cf-474b-8d34-2a95d323153c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
